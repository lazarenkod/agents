---
name: dora-metrics
description: DORA (DevOps Research and Assessment) metrics framework for measuring and improving software delivery performance. Use when establishing delivery metrics, benchmarking team performance, or optimizing CI/CD processes.
---

# DORA Metrics Framework

Comprehensive guide to implementing and improving DORA metrics for elite software delivery performance.

## When to Use This Skill

- Establishing baseline delivery performance metrics
- Benchmarking against industry standards
- Identifying bottlenecks in delivery pipeline
- Measuring impact of process improvements
- Executive reporting on delivery velocity
- Team performance optimization

## Core Concepts

### The Four Key Metrics

**1. Deployment Frequency (DF)**
- **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ**: –ö–∞–∫ —á–∞—Å—Ç–æ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç deployments –≤ production
- **Elite**: On-demand (multiple deploys per day)
- **High**: Between once per day and once per week
- **Medium**: Between once per week and once per month
- **Low**: Between once per month and once per six months

**2. Lead Time for Changes (LT)**
- **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ**: –í—Ä–µ–º—è –æ—Ç commit –¥–æ running –≤ production
- **Elite**: Less than one hour
- **High**: Between one day and one week
- **Medium**: Between one week and one month
- **Low**: Between one month and six months

**3. Time to Restore Service (MTTR)**
- **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ**: –í—Ä–µ–º—è –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ—Å–ª–µ incident
- **Elite**: Less than one hour
- **High**: Less than one day
- **Medium**: Between one day and one week
- **Low**: More than one week

**4. Change Failure Rate (CFR)**
- **–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ**: % deployments causing production issues
- **Elite**: 0-15%
- **High**: 16-30%
- **Medium**: 31-45%
- **Low**: 46-60%

## Implementation Guide

### –ò–∑–º–µ—Ä–µ–Ω–∏–µ Deployment Frequency

**–ò—Å—Ç–æ—á–Ω–∏–∫–∏ –¥–∞–Ω–Ω—ã—Ö:**
- CI/CD pipeline logs (Jenkins, GitLab CI, GitHub Actions)
- APM tools (Datadog, New Relic)
- Git tags –¥–ª—è releases
- Deployment tracking systems

**–†–∞—Å—á–µ—Ç:**
```
DF = Total Production Deployments / Time Period

–ü—Ä–∏–º–µ—Ä:
- 120 deployments –∑–∞ 30 days
- DF = 4 deployments per day (Elite)
```

**SQL Query Example (–¥–ª—è CI/CD –±–∞–∑—ã):**
```sql
SELECT
    DATE(deployed_at) as deploy_date,
    COUNT(*) as deployments_per_day
FROM deployments
WHERE environment = 'production'
    AND deployed_at >= DATE_SUB(NOW(), INTERVAL 30 DAY)
GROUP BY DATE(deployed_at)
ORDER BY deploy_date DESC;
```

### –ò–∑–º–µ—Ä–µ–Ω–∏–µ Lead Time

**–ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã Lead Time:**
```
Lead Time = Code Commit ‚Üí Production
  ‚îú‚îÄ‚îÄ Coding Time (Commit ‚Üí PR)
  ‚îú‚îÄ‚îÄ Review Time (PR Open ‚Üí Approved)
  ‚îú‚îÄ‚îÄ Build Time (Merge ‚Üí Build Complete)
  ‚îú‚îÄ‚îÄ Test Time (Build ‚Üí Tests Pass)
  ‚îú‚îÄ‚îÄ Deployment Time (Deploy Start ‚Üí Live)
  ‚îî‚îÄ‚îÄ Validation Time (Live ‚Üí Verified)
```

**Tracking Method:**
```python
# –†–∞—Å—á–µ—Ç lead time —á–µ—Ä–µ–∑ Git + CI/CD
lead_time = production_deploy_timestamp - first_commit_timestamp

# –î–µ—Ç–∞–ª—å–Ω–∞—è —Ä–∞–∑–±–∏–≤–∫–∞:
components = {
    'code_time': pr_created - first_commit,
    'review_time': pr_merged - pr_created,
    'ci_time': build_complete - pr_merged,
    'deploy_time': prod_live - build_complete
}
```

### –ò–∑–º–µ—Ä–µ–Ω–∏–µ MTTR

**–ü—Ä–æ—Ü–µ—Å—Å –∏–∑–º–µ—Ä–µ–Ω–∏—è:**
```
MTTR = Time Incident Resolved - Time Incident Detected

–≠—Ç–∞–ø—ã:
1. Detection: –ö–æ–≥–¥–∞ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –ø—Ä–æ–±–ª–µ–º–∞ (monitoring alert)
2. Response: –ö–æ–≥–¥–∞ –∫–æ–º–∞–Ω–¥–∞ –Ω–∞—á–∞–ª–∞ —Ä–∞–±–æ—Ç—É
3. Diagnosis: –ö–æ–≥–¥–∞ –ø—Ä–∏—á–∏–Ω–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∞
4. Resolution: –ö–æ–≥–¥–∞ –ø—Ä–æ–±–ª–µ–º–∞ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∞
5. Verification: –ö–æ–≥–¥–∞ —Ä–µ—à–µ–Ω–∏–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ
```

**Incident Tracking:**
| Incident ID | Detected | Resolved | Duration | Severity | Root Cause |
|-------------|----------|----------|----------|----------|------------|
| INC-001 | 10:15 | 10:42 | 27 min | P1 | Cache failure |
| INC-002 | 14:30 | 16:15 | 1h 45m | P0 | DB connection pool |

### –ò–∑–º–µ—Ä–µ–Ω–∏–µ Change Failure Rate

**–§–æ—Ä–º—É–ª–∞:**
```
CFR = (Failed Deployments / Total Deployments) √ó 100%

Failed Deployment –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –∫–∞–∫:
- Rollback –≤ —Ç–µ—á–µ–Ω–∏–µ 24 —á–∞—Å–æ–≤
- Hotfix deployment —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ
- P0/P1 incident caused by deployment
- Degraded performance requiring intervention
```

**Tracking Table:**
| Deploy ID | Date | Status | Rollback | Incident | Notes |
|-----------|------|--------|----------|----------|-------|
| D-1001 | 2024-01-15 | Success | No | - | Clean deploy |
| D-1002 | 2024-01-15 | Failed | Yes | INC-003 | API errors |
| D-1003 | 2024-01-16 | Success | No | - | |

## Metrics Dashboard Template

### Executive Dashboard (Monthly)

```markdown
# DORA Metrics Report - [–ú–µ—Å—è—Ü –ì–æ–¥]

## Summary

| Metric | Current | Previous | Target | Status |
|--------|---------|----------|--------|--------|
| Deployment Frequency | 3.2/day | 2.8/day | 4/day | üü° |
| Lead Time | 6.5 hours | 8.2 hours | <4 hours | üü° |
| MTTR | 42 minutes | 65 minutes | <30 min | üü° |
| Change Failure Rate | 8% | 12% | <5% | üü¢ |

**Performance Level**: High (trending Elite)

## Trends

[–ì—Ä–∞—Ñ–∏–∫–∏ showing 3-month trend lines]

## Key Insights

**Improvements:**
- ‚úÖ CFR —Å–Ω–∏–∑–∏–ª—Å—è –Ω–∞ 33% –±–ª–∞–≥–æ–¥–∞—Ä—è improved testing
- ‚úÖ MTTR improvement —á–µ—Ä–µ–∑ better runbooks
- ‚úÖ Lead time reduction from pipeline optimization

**Areas for Focus:**
- üéØ Increase deployment frequency to 4+/day
- üéØ Reduce lead time below 4 hours
- üéØ Achieve <30min MTTR consistently

## Action Items

1. [Action 1]: [Owner] - [Due date]
2. [Action 2]: [Owner] - [Due date]
```

## Improvement Strategies

### Improving Deployment Frequency

**Tactics:**
1. **Automate Everything**: Remove manual approval steps
2. **Small Batch Size**: Deploy smaller, incremental changes
3. **Feature Flags**: Decouple deploy from release
4. **Continuous Deployment**: Auto-deploy on green builds
5. **Remove Bottlenecks**: Eliminate deployment windows

**Anti-patterns:**
- ‚ùå Weekly release trains
- ‚ùå Manual approval gates
- ‚ùå Large batch releases
- ‚ùå Change advisory boards for every change

### Improving Lead Time

**Tactics:**
1. **Trunk-Based Development**: Reduce branch lifetime
2. **Fast CI Pipelines**: Optimize build/test time
3. **Parallel Testing**: Run tests concurrently
4. **Automated Code Review**: Static analysis pre-PR
5. **Reduce WIP**: Focus on completing work

**Bottleneck Analysis:**
```
Value Stream Map:
Commit ‚Üí [2h] ‚Üí PR Review ‚Üí [4h] ‚Üí CI Build ‚Üí [30m] ‚Üí Deploy ‚Üí [15m] ‚Üí Live
         ^^^^                ^^^^
    Optimization targets
```

### Improving MTTR

**Tactics:**
1. **Comprehensive Monitoring**: Detect issues faster
2. **Automated Rollback**: One-click revert capability
3. **Runbooks**: Document common issues
4. **Incident Response Process**: Clear escalation paths
5. **Practice Chaos Engineering**: Build muscle memory

**Incident Response Checklist:**
```markdown
- [ ] Incident detected and logged
- [ ] On-call engineer paged
- [ ] Initial assessment (5 min)
- [ ] War room established if needed
- [ ] Mitigation in progress
- [ ] Service restored
- [ ] Root cause identified
- [ ] Post-mortem scheduled
```

### Improving Change Failure Rate

**Tactics:**
1. **Comprehensive Testing**: Unit, integration, E2E, performance
2. **Canary Deployments**: Gradual rollout with monitoring
3. **Feature Flags**: Kill switch for problematic features
4. **Pre-Production Testing**: Staging environment parity
5. **Post-Deployment Monitoring**: Automated health checks

**Quality Gates:**
```yaml
pipeline:
  - unit_tests: >80% coverage
  - integration_tests: all pass
  - security_scan: no critical vulnerabilities
  - performance_test: <500ms p95 latency
  - canary_deploy: 5% traffic for 30min
  - full_deploy: monitor error rates
  - auto_rollback: if error rate >2%
```

## Benchmarking

### Industry Performance Levels

| Level | DF | Lead Time | MTTR | CFR |
|-------|----|-----------| -----|-----|
| **Elite** | On-demand (multiple/day) | <1 hour | <1 hour | 0-15% |
| **High** | Daily to weekly | 1 day - 1 week | <1 day | 16-30% |
| **Medium** | Weekly to monthly | 1 week - 1 month | 1 day - 1 week | 31-45% |
| **Low** | Monthly to 6 months | 1-6 months | >1 week | 46-60% |

### Company Size Context

**Startups (<50 people):**
- Target: High to Elite
- Focus: Speed, experimentation
- Common challenges: Quality processes

**Scale-ups (50-500):**
- Target: High
- Focus: Scaling practices, maintaining velocity
- Common challenges: Coordination overhead

**Enterprises (500+):**
- Target: Medium to High
- Focus: Consistency, compliance, coordination
- Common challenges: Organizational complexity

## Tools & Integration

### Recommended Tools

**CI/CD Analytics:**
- GitHub Actions insights
- GitLab CI/CD analytics
- CircleCI Insights
- Jenkins Blue Ocean

**APM & Monitoring:**
- Datadog DORA dashboard
- New Relic deployment tracking
- Honeycomb observability
- Grafana dashboards

**Incident Management:**
- PagerDuty analytics
- Opsgenie reporting
- Incident.io metrics

**All-in-One:**
- Sleuth.io (DORA-focused)
- LinearB
- Code Climate Velocity
- Swarmia

## Templates

–°–º. `assets/` –¥–ª—è:
- `dora-dashboard-template.md` - –®–∞–±–ª–æ–Ω –¥–∞—à–±–æ—Ä–¥–∞
- `metrics-tracking-sheet.xlsx` - –¢–∞–±–ª–∏—Ü–∞ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è
- `improvement-plan-template.md` - –ü–ª–∞–Ω —É–ª—É—á—à–µ–Ω–∏–π
- `executive-report-template.md` - Executive –æ—Ç—á–µ—Ç

## Common Pitfalls

‚ùå **Focusing on One Metric**: –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –æ–¥–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ –∑–∞ —Å—á–µ—Ç –¥—Ä—É–≥–∏—Ö
‚úÖ –ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –≤—Å–µ 4 –º–µ—Ç—Ä–∏–∫–∏

‚ùå **Vanity Metrics**: Tracking –±–µ–∑ action plans
‚úÖ Metrics ‚Üí Insights ‚Üí Actions ‚Üí Improvement

‚ùå **Blame Culture**: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å metrics –¥–ª—è –Ω–∞–∫–∞–∑–∞–Ω–∏—è
‚úÖ Blameless culture, focus –Ω–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —É–ª—É—á—à–µ–Ω–∏—è—Ö

‚ùå **Inconsistent Measurement**: Changing definitions —á–∞—Å—Ç–æ
‚úÖ Stable definitions, consistent tracking

## Success Criteria

- **Baseline Established**: 3 months consistent data
- **Trend Analysis**: Month-over-month improvement visibility
- **Team Awareness**: Everyone understands metrics
- **Actionable Insights**: Metrics drive specific improvements
- **Elite Performance**: Achieving elite in 2-3 metrics
